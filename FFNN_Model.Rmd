---
title: "FFNN Model"
author: "Jake Smith"
date: "7/8/2019"
---

Fit a rough feed-forward neural network.

The training/validation set components X.train, Y.train, X.validation, and Y.validation are generated by "training_set_preparation.Rmd".

```{r setup, include=FALSE}
require(keras)
```

```{r}
# define the model using fairly arbitrary parameters
model <- keras_model_sequential()
model %>%
  layer_dense(units = 1000, activation = "relu", input_shape = dim(X.train)[2]) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 1000, activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = dim(Y.train)[2], activation = "softmax")
  
# compile the model
compile(model, loss = "categorical_crossentropy", optimizer = optimizer_adadelta(), metrics = "accuracy")

# fit the model
fit(model, x = X.train, y = Y.train, epochs = 10, batch_size = 50, validation_data = list(X.validation, Y.validation), verbose = 1)
```

The validation accuracies here after 10 epochs are in line with the validation accuracy of a simple multinomial regression model (26-27%). We could likely improve more if we let it fit more epochs, as the train and validation losses have yet to diverge. Since we are targetting a significant improvement over these accuracies, though, we will move straight on to testing more complex architechtures.


