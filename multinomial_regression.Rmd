---
title: "Multinomial Regression"
author: "Jake Smith"
date: "7/8/2019"
output: html_document
---

Start by fitting a multinomial regression model with lasso regularization.

The training/validation set components X.train, Y.train, X.validation, and Y.validation are generated by "training_set_preparation.Rmd".

```{r setup, include=FALSE}
require(glmnet)
require(foreach)
```

```{r}
# fit a multinomial regression model
# we will allow the glmnet function to select the range of regularization penalties lambda
# we will set the elasticnet paramter alpha to 1 (lasso regularization)
# finally, we need to drop the first column of Y.train (no class), as it is zero-filled
glm.lasso <- glmnet(x = X.train, y = Y.train[, 2:11], family = "multinomial", alpha = 1, standardize = FALSE)

# predict classes for the validation set
validation.predictions <- predict(glm.lasso, X.validation, type = "class")


## compare the predicted values for the validation set to the known values
# reverse one-hot encoding of Y.validation
validation.observed <- foreach(row = 1:dim(Y.validation[, 2:11])[1], .combine = c) %do% {
  which(Y.validation[row, 2:11] == max(Y.validation[row, 2:11]))
}

# calculate validation accuracy 
validation.accuracy <- apply(validation.predictions, MARGIN = 2, 
                    FUN = function(x) {sum(x == validation.observed) / length(x)})

# plot validation accuracy versus log(lambda)
ggplot() + geom_point(aes(x = log(glm.lasso$lambda), y = validation.accuracy)) + xlab("log(lambda)") + ylab("accuracy")
```

We look to be hitting 26-27% validation accuracy with this method.

